{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies**"
      ],
      "metadata": {
        "id": "eZC6HhmM-I1v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "z7uWB8hTjq6t"
      },
      "outputs": [],
      "source": [
        "# !pip install langchain langchain-community langchain-core faiss-cpu chromadb pypdf sentence-transformers\n",
        "# !pip install transformers accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installs all the libraries needed:\n",
        "\n",
        "LangChain → framework for chaining components.\n",
        "\n",
        "faiss-cpu / chromadb → vector databases for similarity search.\n",
        "\n",
        "pypdf → load PDF documents.\n",
        "\n",
        "sentence-transformers → for embeddings.\n",
        "\n",
        "transformers, accelerate, bitsandbytes → for loading and running TinyLLaMA efficiently on GPU."
      ],
      "metadata": {
        "id": "DHArGGCI-2K6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load TinyLLaMA Model**"
      ],
      "metadata": {
        "id": "ladGkmFK-Oxx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noRTq4Irob4E",
        "outputId": "1d5dcac0-6caf-4385-c96d-38fde2ebe8b0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loads TinyLLaMA 1B Chat model from Hugging Face.\n",
        "\n",
        "Tokenizer converts text → tokens (numbers).\n",
        "\n",
        "Model generates language outputs (text) from input tokens.\n",
        "\n",
        "torch_dtype=torch.float16 and device_map=\"auto\" make it GPU-efficient."
      ],
      "metadata": {
        "id": "gjwIezWz-8pP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Wrap Model in LangChain LLM Interface**"
      ],
      "metadata": {
        "id": "XY5ZS8cP-TDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    do_sample=True\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6GJfC6DojNI",
        "outputId": "76ffa5de-6bfc-462c-81f9-e5e2c4e10a92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "/tmp/ipython-input-3652526408.py:12: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=pipe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wraps the Hugging Face model into a LangChain-compatible LLM object.\n",
        "\n",
        "The pipeline() defines how the model will generate text (max tokens, randomness).\n",
        "\n",
        "llm now behaves like any LangChain LLM — you can plug it into chains.\n",
        "\n",
        "Think of this as the bridge connecting TinyLLaMA ↔ LangChain."
      ],
      "metadata": {
        "id": "nF8Lkqvl_EMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Split Documents**"
      ],
      "metadata": {
        "id": "CiG_YRLp-XZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"/content/FineTuningLLMs.pdf\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "splits = splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "hdXTh-9lqdi7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyPDFLoader reads all pages of your PDF into LangChain Document objects.\n",
        "\n",
        "Each Document has:\n",
        "\n",
        "page_content → the text,\n",
        "\n",
        "metadata → file/page info.\n",
        "\n",
        "RecursiveCharacterTextSplitter breaks long documents into smaller chunks (500 characters each with 50 overlapping).\n",
        "\n",
        "These chunks will later be embedded & stored in the vector database.\n",
        "\n",
        "Splitting ensures better context retrieval (smaller chunks = more accurate matches)."
      ],
      "metadata": {
        "id": "NHEq6REP_O3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embeddings + Vector Store**"
      ],
      "metadata": {
        "id": "OiP_fYm4-b5f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "db = FAISS.from_documents(splits, embeddings)\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "metadata": {
        "id": "wjfWWwMkzRJD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "HuggingFaceEmbeddings converts text → high-dimensional vectors.\n",
        "\n",
        "FAISS stores these vectors and allows semantic similarity search.\n",
        "\n",
        "db.as_retriever(k=3) retrieves top 3 most similar chunks for any user question.\n",
        "\n",
        "This is your knowledge base — it stores and retrieves context from documents."
      ],
      "metadata": {
        "id": "irsaoDkj_WAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Template**"
      ],
      "metadata": {
        "id": "pePzGdSV-fqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Use the context below to answer the question **strictly using only the given context**.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    template=template,\n",
        "    input_variables=[\"context\", \"question\"],\n",
        ")"
      ],
      "metadata": {
        "id": "cQzYGf_czjVt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defines a prompt template with placeholders {context} and {question}."
      ],
      "metadata": {
        "id": "-Hvn00iO_n0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the RAG Chain (Runnable)**"
      ],
      "metadata": {
        "id": "H7xcaSNU-h8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableMap, RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def combine_docs_with_sources(docs):\n",
        "    combined = \"\"\n",
        "    for i, d in enumerate(docs, start=1):\n",
        "        source = d.metadata.get(\"source\", f\"doc_{i}\")\n",
        "        page = d.metadata.get(\"page\", \"\")\n",
        "        combined += f\"Document {i} (source: {source}, page: {page}):\\n{d.page_content}\\n\\n\"\n",
        "    return combined\n",
        "\n",
        "rag_chain = (\n",
        "    RunnableMap({\n",
        "        \"context\": retriever | combine_docs_with_sources,\n",
        "        \"question\": RunnablePassthrough(),\n",
        "    })\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "Luew5Rzt0EqR"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RunnableMap connects multiple steps together:\n",
        "\n",
        "The retriever finds relevant docs and converts them to text (context).\n",
        "\n",
        "The question passes through as-is.\n",
        "\n",
        "The output of this map is fed into:\n",
        "\n",
        "The prompt → builds final text for the LLM.\n",
        "\n",
        "The llm → generates an answer.\n",
        "\n",
        "StrOutputParser() → extracts clean text output."
      ],
      "metadata": {
        "id": "8b4NLoRc_wti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ask Questions**"
      ],
      "metadata": {
        "id": "_vzkL0pi-rZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is SGD?\"\n",
        "result = rag_chain.invoke(query)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XwvuwgF0G_P",
        "outputId": "92122c6f-5b7d-4c0a-dc86-cf3f3f069a19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Use the context below to answer the question **strictly using only the given context**.\n",
            "\n",
            "Context:\n",
            "Document 1 (source: /content/FineTuningLLMs.pdf, page: 29):\n",
            "5.4.2 Stochastic Gradient Descent (SGD)\n",
            "Stochastic Gradient Descent (SGD) is a variant of Gradient Descent that focuses on reducing computation\n",
            "per iteration.\n",
            "How it Works: SGD updates parameters using a single or few data points at each iteration, intro-\n",
            "\n",
            "Document 2 (source: /content/FineTuningLLMs.pdf, page: 30):\n",
            "When to Use: SGD is ideal for large datasets, incremental learning scenarios, and real-time learning\n",
            "environments where computational resources are limited.\n",
            "5.4.3 Mini-batch Gradient Descent\n",
            "Mini-batch Gradient Descent combines the efficiency of SGD and the stability of batch Gradient Descent,\n",
            "\n",
            "Document 3 (source: /content/FineTuningLLMs.pdf, page: 29):\n",
            "• Sensitive to the choice of learning rate.\n",
            "When to Use: Gradient Descent is best used for small datasets where gradient computation is\n",
            "cheap and simplicity and clarity are preferred.\n",
            "5.4.2 Stochastic Gradient Descent (SGD)\n",
            "\n",
            "\n",
            "\n",
            "Question:\n",
            "What is SGD?\n",
            "\n",
            "Answer:\n",
            "SGD is a variant of Gradient Descent that focuses on reducing computation per iteration. It works by taking a single or few data points at each iteration and updates the parameters using a weighted average of these points, with the weighting determined by the gradient of the loss function at the current sample.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sends a question to the RAG pipeline.\n",
        "\n",
        "The chain retrieves relevant document chunks, injects them into the prompt, and generates a context-aware answer using TinyLLaMA."
      ],
      "metadata": {
        "id": "JZxhU_Bz_4YJ"
      }
    }
  ]
}